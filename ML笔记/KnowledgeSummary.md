# K近邻算法(KNN)

## 工作原理

给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中

## 算法优缺点

## K值的选择对算法的影响

## KNN分类算法流程

•对未知类别的数据集中的每个点依次执行以下操作

•计算已知类别数据集众多点与当前点之间的距离

•按照距离递增次序排序

•选取与当前点距离最小的k个点

•群定前k个点所在类别的出现频率

•返回前k个点出现频率最高的类别作为当前点的预测分类

## 阐述一下你所知道的机器学习相关的度量

https://blog.csdn.net/v_july_v/article/details/8203674

## KD树和BBF算法，SIFT特征匹配算法

 Kd-树是K-dimension tree的缩写，是对数据点在k维空间中划分的一种数据结构，主要应用于多维空间关键数据的搜索（如：范围搜索和最近邻搜索），本质上说，Kd-树就是一种平衡二叉树。

对于n个实例的k维数据来说，建立kd-tree的时间复杂度为O(k*n*logn)。

KD树算法流程：

![img](C:\Users\18086873\Desktop\机器学习资料\KDTree.png)

![img](C:\Users\18086873\Desktop\机器学习资料\KDTree1.png)



## 如何设计一个比较两篇文章相似性的算法？simhash算法的介绍

通过过距离函数在高维矢量之间进行相似性检索，如何快速而准确地找到查询点的近邻，不少人提出了很多高维空间索引结构和近似查询的算法。

1. **索引结构中相似性查询有两种基本的方式：**

   - 范围查询，范围查询时给定查询点和查询距离阈值，从数据集中查找所有与查询点距离小于阈值的数据

   - K近邻查询，就是给定查询点及正整数K，从数据集中找到距离查询点最近的K个数据，当K=1时，它就是最近邻查询

2. **针对特征点匹配：**

- 线性扫描，也就是我们常说的穷举搜索，依次计算样本集E中每个样本到输入实例点的距离，然后抽取出计算出来的最小距离的点即为最近邻点。此种办法简单直白，但当样本集或训练集很大时，它的缺点就立马暴露出来了，举个例子，在物体识别的问题中，可能有数千个甚至数万个SIFT特征点，而去一一计算这成千上万的特征点与输入实例点的距离，明显是不足取的。

- 构建数据索引，因为实际数据一般都会呈现簇状的聚类形态，因此我们想到建立数据索引，然后再进行快速匹配。索引树是一种树结构索引方法，其基本思想是对搜索空间进行层次划分。根据划分的空间是否有混叠可以分为Clipping和Overlapping两种。前者划分空间没有重叠，其代表就是k-d树；后者划分空间相互有交叠，其代表为R树。

  

# 贝叶斯分类器Bayesian Classifier

## 信息增益相关概念

设有随机变量(X,Y),其联合概率分布为：
$$
P(X=x_i, Y=y_j)=p_{ij}, i=1,\cdots{n}; j=1,\cdots{m}
$$

- 条件熵H(Y|X)：表示在己知随机变量X的条件下随机变量Y的不确定性，定义为X给定条件下Y的条件概率分布的熵对X的数学期望

  

$$
H(Y|X) = \sum_{i=1}^np_iH(Y|X_i)
$$

## 贝叶斯算法处理流程

![image-20200512150124561](C:\Users\18086873\Desktop\机器学习资料\bayes.png)

